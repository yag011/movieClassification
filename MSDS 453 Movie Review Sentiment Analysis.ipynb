{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\yabon\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\yabon\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\yabon\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\yabon\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\yabon\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\yabon\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\yabon\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\yabon\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\yabon\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\yabon\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\yabon\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\yabon\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "\n",
    "import re,string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from sklearn.manifold import MDS\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import os\n",
    "\n",
    "from gensim.models import Word2Vec,LdaMulticore, TfidfModel\n",
    "from gensim import corpora\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean doc\n",
    "\n",
    "def clean_doc(doc): \n",
    "    #split document into individual words\n",
    "    tokens=doc.split()\n",
    "    re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
    "    # remove punctuation from each word\n",
    "    tokens = [re_punc.sub('', w) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 2]\n",
    "    #lowercase all words\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]         \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_list(lines, filename):\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Counter()\n",
    "\n",
    "os.chdir(r'C:\\Users\\yabon\\Downloads')\n",
    "\n",
    "\n",
    "#read in class corpus csv into python\n",
    "data=pd.read_csv('combined_corpus_with_labels.csv')\n",
    "\n",
    "#create empty list to store text documents titles\n",
    "titles=[]\n",
    "\n",
    "#for loop which appends the DSI title to the titles list\n",
    "for i in range(0,len(data)):\n",
    "    temp_text=data['DSI_Title'].iloc[i]\n",
    "    titles.append(temp_text)\n",
    "\n",
    "#create empty list to store text documents\n",
    "text_body=[]\n",
    "\n",
    "#for loop which appends the text to the text_body list\n",
    "for i in range(0,len(data)):\n",
    "    temp_text=data['Text'].iloc[i]\n",
    "    text_body.append(temp_text)\n",
    "\n",
    "#Note: the text_body is the unprocessed list of documents read directly form \n",
    "#the csv.\n",
    "    \n",
    "#empty list to store processed documents\n",
    "processed_text=[]\n",
    "#for loop to process the text to the processed_text list\n",
    "for i in text_body:\n",
    "    text=clean_doc(i)\n",
    "    processed_text.append(text)\n",
    "    vocab.update(text)\n",
    "\n",
    "#Note: the processed_text is the PROCESSED list of documents read directly form \n",
    "#the csv.  Note the list of words is separated by commas.\n",
    "\n",
    "\n",
    "#stitch back together individual words to reform body of text\n",
    "final_processed_text=[]\n",
    "\n",
    "for i in processed_text:\n",
    "    temp_DSI=i[0]\n",
    "    for k in range(1,len(i)):\n",
    "        temp_DSI=temp_DSI+' '+i[k]\n",
    "    final_processed_text.append(temp_DSI)\n",
    "    \n",
    "# only keep tokens with >=5 occurences\n",
    "min_occurence = 5\n",
    "tokens  = [k for k,c in vocab.items() if c>= min_occurence]\n",
    "save_list(tokens, 'vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 66 entries, 0 to 65\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  66 non-null     int64 \n",
      " 1   DSI_Title   66 non-null     object\n",
      " 2   Text        66 non-null     object\n",
      " 3   category    66 non-null     int64 \n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 2.2+ KB\n"
     ]
    }
   ],
   "source": [
    "# Info on our data\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare labels\n",
    "labels = data['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load vocabulary\n",
    "file = open('vocab.txt','r')\n",
    "text = file.read()\n",
    "file.close()\n",
    "vocab = set(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\yabon\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yabon\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\yabon\\anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.6854 - accuracy: 0.5254\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.3341 - accuracy: 1.0000\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.1755 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.0920 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.0469 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.0243 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.0127 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.0072 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.0042 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 1/10\n",
      " - 0s - loss: 5.8730e-04 - accuracy: 1.0000\n",
      "Epoch 2/10\n",
      " - 0s - loss: 3.6882e-04 - accuracy: 1.0000\n",
      "Epoch 3/10\n",
      " - 0s - loss: 2.5998e-04 - accuracy: 1.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yabon\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:23: DeprecationWarning: time.clock has been deprecated in Python 3.3 and will be removed from Python 3.8: use time.perf_counter or time.process_time instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10\n",
      " - 0s - loss: 1.9118e-04 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      " - 0s - loss: 1.4428e-04 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      " - 0s - loss: 1.1364e-04 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      " - 0s - loss: 9.4207e-05 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      " - 0s - loss: 7.9173e-05 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      " - 0s - loss: 6.9806e-05 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      " - 0s - loss: 6.1458e-05 - accuracy: 1.0000\n",
      "Epoch 1/10\n",
      " - 0s - loss: 1.5138e-10 - accuracy: 1.0000\n",
      "Epoch 2/10\n",
      " - 0s - loss: 1.2602e-10 - accuracy: 1.0000\n",
      "Epoch 3/10\n",
      " - 0s - loss: 1.0614e-10 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      " - 0s - loss: 9.4719e-11 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      " - 0s - loss: 8.5217e-11 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      " - 0s - loss: 7.8047e-11 - accuracy: 1.0000\n",
      "Epoch 7/10\n",
      " - 0s - loss: 7.3007e-11 - accuracy: 1.0000\n",
      "Epoch 8/10\n",
      " - 0s - loss: 6.8484e-11 - accuracy: 1.0000\n",
      "Epoch 9/10\n",
      " - 0s - loss: 6.5678e-11 - accuracy: 1.0000\n",
      "Epoch 10/10\n",
      " - 0s - loss: 6.2894e-11 - accuracy: 1.0000\n",
      "Epoch 1/10\n",
      " - 0s - loss: 0.6718 - accuracy: 1.0000\n",
      "Epoch 2/10\n",
      " - 0s - loss: 0.6709 - accuracy: 1.0000\n",
      "Epoch 3/10\n",
      " - 0s - loss: 0.6694 - accuracy: 1.0000\n",
      "Epoch 4/10\n",
      " - 0s - loss: 0.6685 - accuracy: 1.0000\n",
      "Epoch 5/10\n",
      " - 0s - loss: 0.6668 - accuracy: 1.0000\n",
      "Epoch 6/10\n",
      " - 0s - loss: 0.6657 - accuracy: 0.9831\n",
      "Epoch 7/10\n",
      " - 0s - loss: 0.6642 - accuracy: 0.9831\n",
      "Epoch 8/10\n",
      " - 0s - loss: 0.6629 - accuracy: 0.8475\n",
      "Epoch 9/10\n",
      " - 0s - loss: 0.6615 - accuracy: 0.7119\n",
      "Epoch 10/10\n",
      " - 0s - loss: 0.6601 - accuracy: 0.6102\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>binary</th>\n",
       "      <th>count</th>\n",
       "      <th>tfidf</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.285714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.195724</td>\n",
       "      <td>0.489018</td>\n",
       "      <td>0.582758</td>\n",
       "      <td>0.434876</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     binary     count     tfidf      freq\n",
       "0  1.000000  1.000000  1.000000  0.285714\n",
       "1  2.195724  0.489018  0.582758  0.434876"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split into train/test and obtain result for each mode\n",
    "X_train, X_test, y_train, y_test = train_test_split(final_processed_text, labels.values, test_size=0.1)\n",
    "mode = ['binary', 'count', 'tfidf', 'freq']\n",
    "n_repeats = 10\n",
    "results = DataFrame()\n",
    "tokenizer = create_tokenizer(X_train)\n",
    "\n",
    "#Create model to use in all evaluations\n",
    "model = Sequential()\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics=['accuracy'])\n",
    "\n",
    "#iterate through each mode\n",
    "for m in mode:\n",
    "    time_start = time.clock()\n",
    "    train = tokenizer.texts_to_matrix(X_train, mode = m)\n",
    "    test = tokenizer.texts_to_matrix(X_test, mode = m)\n",
    "    model.fit(train, y_train, epochs=10, verbose=2)\n",
    "    scores = list()\n",
    "    _, acc = model.evaluate(test, y_test, verbose=0)  \n",
    "    scores.append(acc)\n",
    "    scores.append(time.clock()-time_start)\n",
    "    results[m] = scores\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiments(review, vocab, tokenizer, model):\n",
    "    tokens = clean_doc(review)\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    line = ' '.join(tokens)\n",
    "    encoded = tokenizer.texts_to_matrix([line], mode = 'count')\n",
    "    yhat = model.predict(encoded, verbose=0)\n",
    "    percent_pos = yhat[0,0]\n",
    "    if round(percent_pos)==0:\n",
    "        return 'ACTION/ADVENTURE'\n",
    "    return 'OTHER'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ACTION/ADVENTURE', 'ACTION/ADVENTURE']\n",
      "['OTHERS', 'OTHER']\n",
      "['OTHERS', 'OTHER']\n",
      "['OTHERS', 'OTHER']\n",
      "['OTHERS', 'OTHER']\n",
      "['OTHERS', 'OTHER']\n",
      "['OTHERS', 'OTHER']\n",
      "['ACTION/ADVENTURE', 'ACTION/ADVENTURE']\n",
      "['OTHERS', 'OTHER']\n",
      "['ACTION/ADVENTURE', 'ACTION/ADVENTURE']\n"
     ]
    }
   ],
   "source": [
    "#predict sentiments from 10 randomly selected files in original corpus\n",
    "results = DataFrame()\n",
    "for i in range(10):\n",
    "    scores = list()\n",
    "    num = random.randint(0,len(data))\n",
    "    temp_text=data['Text'].iloc[num]\n",
    "    cat = data['category'].iloc[num]\n",
    "    if cat==0:\n",
    "        scores.append('ACTION/ADVENTURE')\n",
    "    else:\n",
    "        scores.append('OTHERS')\n",
    "    scores.append(predict_sentiments(temp_text, vocab, tokenizer, model))\n",
    "    print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ACTION/ADVENTURE', 'ACTION/ADVENTURE']\n",
      "['ACTION/ADVENTURE', 'ACTION/ADVENTURE']\n",
      "['OTHERS', 'OTHER']\n",
      "['OTHERS', 'OTHER']\n"
     ]
    }
   ],
   "source": [
    "#predict sentiments from 10 randomly selected files in untouched corpus\n",
    "test_data=pd.read_csv('Random_samples.csv')\n",
    "test_data.dropna()\n",
    "for i in range(len(test_data)):\n",
    "    scores = list()\n",
    "    temp_text=test_data['Title'].iloc[i]\n",
    "    cat = test_data['Category'].iloc[i]\n",
    "    if cat==0:\n",
    "        scores.append('ACTION/ADVENTURE')\n",
    "    else:\n",
    "        scores.append('OTHERS')\n",
    "    scores.append(predict_sentiments(temp_text, vocab, tokenizer, model))\n",
    "    print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
